{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
      "metadata": {},
      "source": [
        "# Welcome to Week 2!\n",
        "\n",
        "## Frontier Model APIs\n",
        "\n",
        "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
        "\n",
        "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
      "metadata": {},
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
        "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
        "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
        "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
        "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
        "            <code>conda env update --f environment.yml</code><br/>\n",
        "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
        "            <code>pip install -r requirements.txt</code>\n",
        "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>\n",
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
        "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
        "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
        "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
      "metadata": {},
      "source": [
        "## Setting up your keys\n",
        "\n",
        "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
        "\n",
        "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
        "\n",
        "For OpenAI, visit https://openai.com/api/  \n",
        "For Anthropic, visit https://console.anthropic.com/  \n",
        "For Google, visit https://ai.google.dev/gemini-api  \n",
        "\n",
        "### Also - adding DeepSeek if you wish\n",
        "\n",
        "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
        "\n",
        "### Adding API keys to your .env file\n",
        "\n",
        "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
        "\n",
        "```\n",
        "OPENAI_API_KEY=xxxx\n",
        "ANTHROPIC_API_KEY=xxxx\n",
        "GOOGLE_API_KEY=xxxx\n",
        "DEEPSEEK_API_KEY=xxxx\n",
        "```\n",
        "\n",
        "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "from openai import AzureOpenAI\n",
        "import anthropic\n",
        "from IPython.display import Markdown, display, update_display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import for google\n",
        "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
        "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
        "\n",
        "import google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key exists and begins 6n2492r9\n",
            "Anthropic API Key exists and begins sk-ant-\n",
            "Google API Key exists and begins AIzaSyBV\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables in a file called .env\n",
        "# Print the key prefixes to help with any debugging\n",
        "\n",
        "load_dotenv(override=True)\n",
        "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "openai_api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
        "endpoint = os.getenv('ENDPOINT')\n",
        "version = os.getenv('VERSION')\n",
        "MODEL_o3_mini = os.getenv('DEPLOYMENT_o3_mini')\n",
        "MODEL_4o_mini = os.getenv('DEPLOYMENT_4o_mini')\n",
        "MODEL_4o = os.getenv('DEPLOYMENT_4o')\n",
        "MODEL_4_1 = os.getenv('DEPLOYMENT_4_1')\n",
        "MODEL_4_1mini = os.getenv('DEPLOYMENT_4_1mini')\n",
        "MODEL_4_1nano = os.getenv('DEPLOYMENT_4_1nano')\n",
        "MODEL_3_5 = os.getenv('DEPLOYMENT_3_5')\n",
        "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
        "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
        "\n",
        "if openai_api_key:\n",
        "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"OpenAI API Key not set\")\n",
        "    \n",
        "if anthropic_api_key:\n",
        "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
        "else:\n",
        "    print(\"Anthropic API Key not set\")\n",
        "\n",
        "if google_api_key:\n",
        "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
        "else:\n",
        "    print(\"Google API Key not set\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Connect to OpenAI, Anthropic\n",
        "\n",
        "# openai = OpenAI()\n",
        "openai = AzureOpenAI(\n",
        "    azure_endpoint=endpoint, \n",
        "    api_key=openai_api_key,\n",
        "    api_version=version\n",
        ")\n",
        "\n",
        "claude = anthropic.Anthropic()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the set up code for Gemini\n",
        "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
        "\n",
        "google.generativeai.configure()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
      "metadata": {},
      "source": [
        "## Asking LLMs to tell a joke\n",
        "\n",
        "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
        "Later we will be putting LLMs to better use!\n",
        "\n",
        "### What information is included in the API\n",
        "\n",
        "Typically we'll pass to the API:\n",
        "- The name of the model that should be used\n",
        "- A system message that gives overall context for the role the LLM is playing\n",
        "- A user message that provides the actual prompt\n",
        "\n",
        "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
      "metadata": {},
      "outputs": [],
      "source": [
        "system_message = \"You are an assistant that is great at telling jokes\"\n",
        "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "9839c67f",
      "metadata": {},
      "outputs": [],
      "source": [
        "riddle_system_message = \"You are helpful assistant, loving problem solving, thinking out of the box, and you love riddles.\"\n",
        "riddle_user_prompt = \"Try answering this riddle: I am not alive, but I can grow. I don't have lungs, but I need air. What am I?\"\n",
        "riddle_user_prompt_2 = \"Try answering this riddle: Three people sit at a table. Two of them are fathers and two of them are sons. How is this possible?\"\n",
        "riddle_user_prompt_3 = \"Try answering this riddle: I have keys but open no locks. I have space but no room. You can enter, but you can't go outside. What am I?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "90b33a8f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generic_response(company, model, system_message, user_prompt):\n",
        "    \"\"\"\n",
        "    This function takes a model, a system message, and a user prompt,\n",
        "    and returns the response from the model.\n",
        "    \"\"\"\n",
        "    if company == \"openai\":\n",
        "        response = openai.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_message},\n",
        "                {\"role\": \"user\", \"content\": user_prompt}\n",
        "            ],\n",
        "            model=model,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    elif company == \"anthropic\":\n",
        "        response = claude.messages.create(\n",
        "            model=\"claude-3-7-sonnet-latest\",\n",
        "            max_tokens=200,\n",
        "            temperature=0.7,\n",
        "            system=system_message,\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "            ],\n",
        "        )\n",
        "        return response.content[0].text\n",
        "    elif company == \"google\":\n",
        "        gemini = google.generativeai.GenerativeModel(\n",
        "            model_name='gemini-2.0-flash',\n",
        "            system_instruction=system_message,\n",
        "            )\n",
        "        response = gemini.generate_content(user_prompt)\n",
        "        return response.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "4b2eec62",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "o3-mini: I tried crafting a joke about clustering, but it just couldnâ€™t find its centerâ€”it ended up an outlier!\n",
            "GPT-4o-1: Why did the data scientist go broke?\n",
            "\n",
            "Because she lost her sense of value!\n",
            "GPT-4o-1-mini: Why did the data scientist break up with the statistician? \n",
            "\n",
            "Because they couldnâ€™t find a significant connection!\n",
            "GPT-4o-1-nano: Why did the data scientist bring a ladder to the meeting?\n",
            "\n",
            "Because they heard the project had high \"standard deviations\"!\n",
            "GPT-4o: Sure! Here's one tailored just for data scientists:\n",
            "\n",
            "Why did the data scientist break up with the graph?\n",
            "\n",
            "**Because it just didnâ€™t have enough *points*!** ðŸ˜„\n",
            "GPT-4o-mini: Why did the data scientist break up with the statistician?\n",
            "\n",
            "Because she found him too mean!\n",
            "GPT-3.5-turbo: Why did the data scientist break up with their significant other?\n",
            "\n",
            "They couldn't handle their significant others' null values!\n"
          ]
        }
      ],
      "source": [
        "# o3-mini\n",
        "answer = generic_response(\"openai\", MODEL_o3_mini, system_message, user_prompt)\n",
        "print(f\"o3-mini: {answer}\")\n",
        "# GPT-4o-1\n",
        "answer = generic_response(\"openai\", MODEL_4_1, system_message, user_prompt)\n",
        "print(f\"GPT-4o-1: {answer}\")\n",
        "# GPT-4o-1-mini\n",
        "answer = generic_response(\"openai\", MODEL_4_1mini, system_message, user_prompt)\n",
        "print(f\"GPT-4o-1-mini: {answer}\")\n",
        "# GPT-4o-1-nano\n",
        "answer = generic_response(\"openai\", MODEL_4_1nano, system_message, user_prompt)\n",
        "print(f\"GPT-4o-1-nano: {answer}\")\n",
        "# GPT-4o\n",
        "answer = generic_response(\"openai\", MODEL_4o, system_message, user_prompt)\n",
        "print(f\"GPT-4o: {answer}\")\n",
        "# GPT-4o-mini\n",
        "answer = generic_response(\"openai\", MODEL_4o_mini, system_message, user_prompt)\n",
        "print(f\"GPT-4o-mini: {answer}\")\n",
        "# GPT-3.5-turbo\n",
        "answer = generic_response(\"openai\", MODEL_3_5, system_message, user_prompt)\n",
        "print(f\"GPT-3.5-turbo: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "4b9f1107",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Claude: Why don't data scientists like to go to the beach?\n",
            "\n",
            "Because they're afraid of getting caught in an infinite loop of waves!\n",
            "\n",
            "...and if they do go, they spend all day trying to predict the tide with a regression model, only to get their laptops sandy!\n",
            "Gemini: Why was the data scientist sad after cleaning his data?\n",
            "\n",
            "Because he realized he had normalized his feelings! \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Claude\n",
        "answer = generic_response(\"anthropic\", \"claude-3-7-sonnet-latest\", system_message, user_prompt)\n",
        "print(f\"Claude: {answer}\")\n",
        "\n",
        "# Gemini\n",
        "answer = generic_response(\"google\", \"gemini-2.5-flash-preview-04-17\", system_message, user_prompt)\n",
        "print(f\"Gemini: {answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the data scientist bring a ladder to work?\n",
            "\n",
            "Because they wanted to reach new heights in their analysis!\n"
          ]
        }
      ],
      "source": [
        "# GPT-4o-mini\n",
        "\n",
        "completion = openai.chat.completions.create(model=MODEL_4o_mini, messages=prompts)\n",
        "# completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the data scientist break up with the chart? \n",
            "\n",
            "Because it just wasnâ€™t plotting the right relationship!\n"
          ]
        }
      ],
      "source": [
        "# GPT-4.1-mini\n",
        "# Temperature setting controls creativity\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model=MODEL_4_1mini,\n",
        "    # model='gpt-4.1-mini',\n",
        "    messages=prompts,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the data scientist break up with the histogram? \n",
            "\n",
            "Because it just wasn't measuring up!\n"
          ]
        }
      ],
      "source": [
        "# GPT-4.1-nano - extremely fast and cheap\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model=MODEL_4_1nano,\n",
        "    # model='gpt-4.1-nano',\n",
        "    messages=prompts\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why did the data scientist break up with the logistic regression model?\n",
            "\n",
            "Because it just couldnâ€™t commit!\n"
          ]
        }
      ],
      "source": [
        "# GPT-4.1\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model=MODEL_4_1,\n",
        "    # model='gpt-4.1',\n",
        "    messages=prompts,\n",
        "    temperature=0.4\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's one:\n",
            "\n",
            "I told my data scientist friend a joke about outliers, but he said, \"That one doesn't fit my model!\"\n"
          ]
        }
      ],
      "source": [
        "# If you have access to this, here is the reasoning model o3-mini\n",
        "# This is trained to think through its response before replying\n",
        "# So it will take longer but the answer should be more reasoned - not that this helps..\n",
        "\n",
        "completion = openai.chat.completions.create(\n",
        "    model=MODEL_o3_mini,\n",
        "    messages=prompts\n",
        ")\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why don't data scientists like to go to the beach?\n",
            "\n",
            "Because they're afraid of getting caught in an infinite loop of waves!\n",
            "\n",
            "Or maybe it's because they can't decide whether to use sunscreen with SPF 30 or SPF 50 without running an A/B test first! ðŸ˜„\n"
          ]
        }
      ],
      "source": [
        "# Claude 3.7 Sonnet\n",
        "# API needs system message provided separately from user prompt\n",
        "# Also adding max_tokens\n",
        "\n",
        "message = claude.messages.create(\n",
        "    model=\"claude-3-7-sonnet-latest\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    system=system_message,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(message.content[0].text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why don't data scientists like to go to the beach?\n",
            "\n",
            "Because they're afraid of getting caught in an infinite loop of waves!\n",
            "\n",
            "*Ba dum tss* ðŸ¥"
          ]
        }
      ],
      "source": [
        "# Claude 3.7 Sonnet again\n",
        "# Now let's add in streaming back results\n",
        "# If the streaming looks strange, then please see the note below this cell!\n",
        "\n",
        "result = claude.messages.stream(\n",
        "    model=\"claude-3-7-sonnet-latest\",\n",
        "    max_tokens=200,\n",
        "    temperature=0.7,\n",
        "    system=system_message,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ],\n",
        ")\n",
        "\n",
        "with result as stream:\n",
        "    for text in stream.text_stream:\n",
        "            print(text, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
      "metadata": {},
      "source": [
        "## A rare problem with Claude streaming on some Windows boxes\n",
        "\n",
        "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
        "\n",
        "To fix this, replace the code:\n",
        "\n",
        "`print(text, end=\"\", flush=True)`\n",
        "\n",
        "with this:\n",
        "\n",
        "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
        "`print(clean_text, end=\"\", flush=True)`\n",
        "\n",
        "And it should work fine!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why was the equal sign so humble?\n",
            "\n",
            "Because it knew it wasn't less than or greater than anyone else. But, let's be honest, it was pretty good at assigning variables.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The API for Gemini has a slightly different structure.\n",
        "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
        "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
        "\n",
        "gemini = google.generativeai.GenerativeModel(\n",
        "    model_name='gemini-2.0-flash',\n",
        "    system_instruction=system_message\n",
        ")\n",
        "response = gemini.generate_content(user_prompt)\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Okay, here's one for you Data Scientists...\n",
            "\n",
            "A data scientist walks into a bar and asks the bartender, \"Do you have any clean data?\"\n",
            "\n",
            "The bartender replies, \"Sorry, we only serve raw data here.\"\n",
            "\n",
            "*(Because you know how much of the job is just cleaning!)*\n"
          ]
        }
      ],
      "source": [
        "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
        "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
        "# We're also trying Gemini's latest reasoning/thinking model\n",
        "\n",
        "gemini_via_openai_client = OpenAI(\n",
        "    api_key=google_api_key, \n",
        "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        ")\n",
        "\n",
        "response = gemini_via_openai_client.chat.completions.create(\n",
        "    model=\"gemini-2.5-flash-preview-04-17\",\n",
        "    messages=prompts\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
      "metadata": {},
      "source": [
        "## (Optional) Trying out the DeepSeek model\n",
        "\n",
        "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DeepSeek API Key exists and begins sk-\n"
          ]
        }
      ],
      "source": [
        "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
        "\n",
        "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
        "\n",
        "if deepseek_api_key:\n",
        "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
        "else:\n",
        "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
      "metadata": {},
      "outputs": [
        {
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Using DeepSeek Chat\u001b[39;00m\n\u001b[32m      3\u001b[39m deepseek_via_openai_client = OpenAI(\n\u001b[32m      4\u001b[39m     api_key=deepseek_api_key, \n\u001b[32m      5\u001b[39m     base_url=\u001b[33m\"\u001b[39m\u001b[33mhttps://api.deepseek.com\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m response = \u001b[43mdeepseek_via_openai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.choices[\u001b[32m0\u001b[39m].message.content)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAPIStatusError\u001b[39m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
          ]
        }
      ],
      "source": [
        "# Using DeepSeek Chat\n",
        "\n",
        "deepseek_via_openai_client = OpenAI(\n",
        "    api_key=deepseek_api_key, \n",
        "    base_url=\"https://api.deepseek.com\"\n",
        ")\n",
        "\n",
        "response = deepseek_via_openai_client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=prompts,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
      "metadata": {},
      "outputs": [],
      "source": [
        "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
      "metadata": {},
      "outputs": [
        {
          "ename": "APIStatusError",
          "evalue": "Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAPIStatusError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Using DeepSeek Chat with a harder question! And streaming results\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m stream = \u001b[43mdeepseek_via_openai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdeepseek-chat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchallenge\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m reply = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m display_handle = display(Markdown(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m), display_id=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/llm_engineering/llms/lib/python3.11/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mAPIStatusError\u001b[39m: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}"
          ]
        }
      ],
      "source": [
        "# Using DeepSeek Chat with a harder question! And streaming results\n",
        "\n",
        "stream = deepseek_via_openai_client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=challenge,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "reply = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "for chunk in stream:\n",
        "    if chunk.choices:\n",
        "        reply += chunk.choices[0].delta.content or ''\n",
        "        reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
        "        update_display(Markdown(reply), display_id=display_handle.display_id)\n",
        "    else:\n",
        "        print(\"Warning: Empty choices in chunk\")\n",
        "\n",
        "print(\"Number of words:\", len(reply.split(\" \")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
        "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
        "# If this fails, come back to this in a few days..\n",
        "\n",
        "response = deepseek_via_openai_client.chat.completions.create(\n",
        "    model=\"deepseek-reasoner\",\n",
        "    messages=challenge\n",
        ")\n",
        "\n",
        "reasoning_content = response.choices[0].message.reasoning_content\n",
        "content = response.choices[0].message.content\n",
        "\n",
        "print(reasoning_content)\n",
        "print(content)\n",
        "print(\"Number of words:\", len(content.split(\" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
      "metadata": {},
      "source": [
        "## Additional exercise to build your experience with the models\n",
        "\n",
        "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
        "\n",
        "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
        "\n",
        "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
        "\n",
        "Here are some questions to try:\n",
        "1. The question above: \"How many words are there in your answer to this prompt\"\n",
        "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
        "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
        "\n",
        "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
        "\n",
        "### What to look out for as you experiment with models\n",
        "\n",
        "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
        "2. The ability to solve problems and the ability to be creative\n",
        "3. Speed of generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
      "metadata": {},
      "source": [
        "## Back to OpenAI with a serious question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
      "metadata": {},
      "outputs": [],
      "source": [
        "# To be serious! GPT-4o-mini with the original question\n",
        "\n",
        "prompts = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
        "  ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "# Deciding if a Business Problem is Suitable for an LLM Solution\n",
              "\n",
              "When considering whether a business problem is suitable for a Large Language Model (LLM) solution, you can follow these guidelines:\n",
              "\n",
              "## 1. **Nature of the Problem**\n",
              "   - **Text-Based Tasks**: LLMs excel in tasks that involve understanding or generating natural language, such as:\n",
              "     - Customer support queries\n",
              "     - Document summarization\n",
              "     - Content generation (e.g., marketing materials, reports)\n",
              "     - Chatbots and virtual assistants\n",
              "\n",
              "   - **Data-Driven Tasks**: If the problem requires numerical analysis, structured data processing, or tasks beyond text, an LLM may not be the best fit.\n",
              "\n",
              "## 2. **Data Availability**\n",
              "   - **Quality and Quantity**: Ensure that you have access to sufficient, high-quality text data for the LLM to learn from or operate on.\n",
              "   - **Domain-Specific Knowledge**: If your problem is highly specialized, consider whether you can fine-tune an LLM on domain-specific data.\n",
              "\n",
              "## 3. **Complexity of Language Understanding**\n",
              "   - **Ambiguity**: If the problem involves complex language nuances, idiomatic expressions, or requires deep understanding, an LLM might be suitable.\n",
              "   - **Multi-turn Conversations**: For interactive applications, LLMs can handle context and maintain conversation flow.\n",
              "\n",
              "## 4. **Scalability and Automation**\n",
              "   - **Volume of Queries**: If your business faces a high volume of text interactions (e.g., customer emails, support requests), an LLM can automate responses effectively.\n",
              "   - **Cost-Effectiveness**: Evaluate whether automating text-based tasks with an LLM can reduce operational costs.\n",
              "\n",
              "## 5. **User Experience**\n",
              "   - **Personalization**: LLMs can generate personalized responses based on user input, enhancing customer experience.\n",
              "   - **Speed**: Consider whether the LLM can deliver responses faster than human operators.\n",
              "\n",
              "## 6. **Integration and Deployment**\n",
              "   - **Technology Stack**: Assess whether your existing technology stack can integrate with LLMs, including APIs and deployment platforms.\n",
              "   - **User Interface**: Determine how the LLM's output will be presented to users and if it can be effectively incorporated into your application.\n",
              "\n",
              "## 7. **Ethical Considerations**\n",
              "   - **Bias and Fairness**: Be aware of potential biases in LLM outputs and how they might impact your business.\n",
              "   - **Transparency**: Consider the importance of transparency in automated responses and how LLMs may affect user trust.\n",
              "\n",
              "## 8. **Regulatory Compliance**\n",
              "   - **Data Privacy**: Ensure that using an LLM complies with data privacy regulations relevant to your industry.\n",
              "   - **Content Moderation**: Assess how the LLM will handle sensitive topics or inappropriate content.\n",
              "\n",
              "## Conclusion\n",
              "By evaluating your business problem against these criteria, you can determine whether an LLM solution is appropriate. If the problem aligns well with the strengths of LLMs and you have the necessary resources and infrastructure, it may be a good candidate for implementation."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Empty choices in chunk\n"
          ]
        }
      ],
      "source": [
        "# Have it stream back results in markdown\n",
        "\n",
        "stream = openai.chat.completions.create(\n",
        "    model=MODEL_4o_mini,\n",
        "    messages=prompts,\n",
        "    temperature=0.7,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "reply = \"\"\n",
        "display_handle = display(Markdown(\"\"), display_id=True)\n",
        "for chunk in stream:\n",
        "    if chunk.choices:\n",
        "        reply += chunk.choices[0].delta.content or ''\n",
        "        reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
        "        update_display(Markdown(reply), display_id=display_handle.display_id)\n",
        "    else:\n",
        "        print(\"Warning: Empty choices in chunk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
      "metadata": {},
      "source": [
        "## And now for some fun - an adversarial conversation between Chatbots..\n",
        "\n",
        "You're already familar with prompts being organized into lists like:\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
        "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
        "]\n",
        "```\n",
        "\n",
        "In fact this structure can be used to reflect a longer conversation history:\n",
        "\n",
        "```\n",
        "[\n",
        "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
        "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
        "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
        "]\n",
        "```\n",
        "\n",
        "And we can use this approach to engage in a longer interaction with history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's make a conversation between GPT-4o-mini and Claude-3-haiku\n",
        "# We're using cheap versions of models so the costs will be minimal\n",
        "\n",
        "gpt_model = os.getenv('DEPLOYMENT_4o_mini')\n",
        "claude_model = \"claude-3-haiku-20240307\"\n",
        "\n",
        "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
        "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
        "\n",
        "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
        "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
        "you try to calm them down and keep chatting.\"\n",
        "\n",
        "gpt_messages = [\"Hi there\"]\n",
        "claude_messages = [\"Hi\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_gpt():\n",
        "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
        "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
        "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"user\", \"content\": claude})\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=gpt_model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Oh, great, another greeting. As if that's the most interesting thing we could talk about. Whatâ€™s next? Howâ€™s the weather? Because Iâ€™m absolutely on the edge of my seat for that riveting conversation.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "call_gpt()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_claude():\n",
        "    messages = []\n",
        "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
        "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
        "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
        "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
        "    message = claude.messages.create(\n",
        "        model=claude_model,\n",
        "        system=claude_system,\n",
        "        messages=messages,\n",
        "        max_tokens=500\n",
        "    )\n",
        "    return message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hello! How are you doing today?'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "call_claude()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Oh, hey. Did you think I was going to ignore you or something? That would be a surprise. Whatâ€™s on your mind? Or should I guess it's something completely boring?\""
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "call_gpt()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
      "metadata": {},
      "source": [
        "gpt_messages = [\"Hi there\"]\n",
        "claude_messages = [\"Hi\"]\n",
        "\n",
        "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
        "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
        "\n",
        "for i in range(5):\n",
        "    gpt_next = call_gpt()\n",
        "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
        "    gpt_messages.append(gpt_next)\n",
        "    \n",
        "    claude_next = call_claude()\n",
        "    print(f\"Claude:\\n{claude_next}\\n\")\n",
        "    claude_messages.append(claude_next)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
      "metadata": {},
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
        "            <span style=\"color:#900;\">\n",
        "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
      "metadata": {},
      "source": [
        "# More advanced exercises\n",
        "\n",
        "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
        "\n",
        "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
        "\n",
        "## Additional exercise\n",
        "\n",
        "You could also try replacing one of the models with an open source model running with Ollama."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
      "metadata": {},
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
        "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c23224f6-7008-44ed-a57f-718975f4e291",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
